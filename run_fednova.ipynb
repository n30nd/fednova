{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINK_DATASET = \"/media/namvq/Data/chest_xray\"\n",
    "LINK_DATASET = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKEND:  Agg\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Partition the data and create the dataloaders.\"\"\"\n",
    "\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, Grayscale, ToTensor\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Chuyển sang backend không cần GUI\n",
    "\n",
    "print('BACKEND: ', matplotlib.get_backend())\n",
    "NUM_WORKERS = 6\n",
    "# def get_custom_dataset(data_path: str = \"/media/namvq/Data/chest_xray\"):\n",
    "#     \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "#     transform = Compose([\n",
    "#         Resize((100, 100)),\n",
    "#         Grayscale(num_output_channels=1),\n",
    "#         ToTensor()\n",
    "#     ])\n",
    "#     trainset = ImageFolder(os.path.join(data_path, 'train'), transform=transform)\n",
    "#     testset = ImageFolder(os.path.join(data_path, 'test'), transform=transform)\n",
    "#     return trainset, testset\n",
    "\n",
    "# def get_custom_dataset(data_path: str = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"):\n",
    "#     \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),  # Kích thước ảnh cho EfficientNet\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "#                              [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "#     ])\n",
    "#     trainset = ImageFolder(os.path.join(data_path, 'train'), transform=transform)\n",
    "#     testset = ImageFolder(os.path.join(data_path, 'test'), transform=transform)\n",
    "#     return trainset, testset\n",
    "\n",
    "# def get_custom_dataset(data_path: str = \"/media/namvq/Data/chest_xray\"):\n",
    "#     \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "#     train_transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),  # Kích thước ảnh cho EfficientNet\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "#                              [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "#     ])\n",
    "#     test_transform = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),  # Kích thước ảnh cho EfficientNet\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "#                              [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "#     ])\n",
    "#     trainset = ImageFolder(os.path.join(data_path, 'train'), transform=train_transform)\n",
    "#     testset = ImageFolder(os.path.join(data_path, 'test'), transform=test_transform)\n",
    "#     return trainset, testset\n",
    "# def get_custom_dataset(data_path: str = \"/home/namvq1/Documents/chest_xray\"):\n",
    "#     \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "#     train_transform = transforms.Compose([\n",
    "#         transforms.Resize((150, 150)),  # Kích thước ảnh cho VGG\n",
    "#         transforms.RandomAffine(degrees=0, shear=10),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.RandomResizedCrop(150, scale=(0.8, 1.0)),\n",
    "#         transforms.RandomAffine(degrees=0, translate=(0.2, 0)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "#                              [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "#     ])\n",
    "#     test_transform = transforms.Compose([\n",
    "#         transforms.Resize((150, 150)),  # Kích thước ảnh cho VGG\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "#                              [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "#     ])\n",
    "#     trainset = ImageFolder(os.path.join(data_path, 'train'), transform=train_transform)\n",
    "#     testset = ImageFolder(os.path.join(data_path, 'test'), transform=test_transform)\n",
    "#     return trainset, testset\n",
    "#/home/namvq1/Documents/chest_xray\n",
    "\n",
    "def get_custom_dataset(data_path: str = LINK_DATASET):\n",
    "    #For resnet \n",
    "    \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(256),  # Kích thước ảnh cho Resnet\n",
    "        transforms.RandomAffine(degrees=0, shear=10),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.2, 0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "                             [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Kích thước ảnh cho VGG\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "                             [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "    ])\n",
    "    trainset = ImageFolder(os.path.join(data_path, 'train'), transform=train_transform)\n",
    "    testset = ImageFolder(os.path.join(data_path, 'test'), transform=test_transform)\n",
    "    return trainset, testset\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset_for_centralized_train(batch_size: int, val_ratio: float = 0.1, seed: int = 42):\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    # Split trainset into trainset and valset\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloader, valloader, testloader\n",
    "\n",
    "\n",
    "def prepare_dataset(num_partitions: int, batch_size: int, val_ratio: float = 0.1, alpha: float = 100, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate non-IID partitions using Dirichlet distribution.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    \n",
    "    # Split trainset into trainset and valset\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "    \n",
    "    # Generate Dirichlet distribution for each class\n",
    "    class_indices = [np.where(train_labels == i)[0] for i in range(len(np.unique(train_labels)))]\n",
    "    partition_indices = [[] for _ in range(num_partitions)]\n",
    "    \n",
    "    for class_idx in class_indices:\n",
    "        np.random.shuffle(class_idx)\n",
    "        proportions = np.random.dirichlet(np.repeat(alpha, num_partitions))\n",
    "        proportions = (np.cumsum(proportions) * len(class_idx)).astype(int)[:-1]\n",
    "        class_partitions = np.split(class_idx, proportions)\n",
    "        for i in range(num_partitions):\n",
    "            partition_indices[i].extend(class_partitions[i])\n",
    "    \n",
    "    # Create Subsets for each partition\n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "    \n",
    "    # Split valset into partitions\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "\n",
    "    valsets = random_split(valset, partition_len_val, torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Create DataLoaders for each partition\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Calculate class distribution for each partition in trainloaders\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "def prepare_partitioned_dataset(num_partitions: int, batch_size: int, val_ratio: float = 0.1, num_labels_each_party: int = 1, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate partitions where each party has a fixed number of labels.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()  # Load datasets\n",
    "\n",
    "    # Split the trainset into trainset and valset based on the validation ratio\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "\n",
    "    # Define partitions: each party has k labels\n",
    "    num_labels = len(np.unique(train_labels))  # Assuming labels are 0 and 1 for binary classification\n",
    "    times = [0 for i in range(num_labels)]\n",
    "    contain = []\n",
    "    #Phan label cho cac client\n",
    "    for i in range(num_partitions):\n",
    "        current = [i%num_labels]\n",
    "        times[i%num_labels] += 1\n",
    "        if num_labels_each_party > 1:\n",
    "            current.append(1-i%num_labels)\n",
    "            times[1-i%num_labels] += 1\n",
    "        contain.append(current)\n",
    "    print(times)\n",
    "    print(contain)\n",
    "    # Create Subsets for each partition\n",
    "\n",
    "    partition_indices = [[] for _ in range(num_partitions)]\n",
    "    for i in range(num_labels):\n",
    "        idx_i = np.where(train_labels == i)[0]  # Get indices of label i in train_labels\n",
    "        idx_i = [trainset.indices[j] for j in idx_i]  # Convert indices to indices in trainset\n",
    "        # #print label of idx_i\n",
    "        # print(\"Label of idx: \", i)\n",
    "        # for j in range(len(idx_i)):\n",
    "        #     idx_in_dataset = trainset.indices[idx_i[j]]\n",
    "        #     print(trainset.dataset.targets[idx_in_dataset])\n",
    "        np.random.shuffle(idx_i)\n",
    "        split = np.array_split(idx_i, times[i])\n",
    "        ids = 0\n",
    "        for j in range(num_partitions):\n",
    "            if i in contain[j]:\n",
    "                partition_indices[j].extend(split[ids])\n",
    "                ids += 1\n",
    "    \n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    # #print label of client 0\n",
    "    # print(\"Client 0\")\n",
    "    # for i in range(len(trainsets[0])):\n",
    "    #     print(trainsets[0][i][1])\n",
    "\n",
    "    # Split valset into partitions\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Create DataLoaders for each partition\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Calculate class distribution for each partition in trainloaders\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "def prepare_imbalance_label_dirichlet(num_partitions: int, batch_size: int, val_ratio: float = 0.1, beta: float = 0.5, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate partitions where each party has a fixed number of labels.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()  # Load datasets\n",
    "\n",
    "    # Split the trainset into trainset and valset based on the validation ratio\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "\n",
    "    # Define partitions: each party has k labels\n",
    "    num_labels = len(np.unique(train_labels))  # Assuming labels are 0 and 1 for binary classification\n",
    "    min_size = 0\n",
    "    min_require_size = 2\n",
    "\n",
    "    N = len(trainset)\n",
    "\n",
    "\n",
    "    while(min_size < min_require_size):\n",
    "        partition_indices = [[] for _ in range(num_partitions)]\n",
    "        for label in range(num_labels):\n",
    "            idx_label = np.where(train_labels == label)[0]\n",
    "            idx_label = [trainset.indices[j] for j in idx_label]\n",
    "            np.random.shuffle(idx_label)\n",
    "\n",
    "            proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "            # proportions = np.array( [p * len(idx_j) < N/num_partitions] for p, idx_j in zip(proportions, partition_indices))\n",
    "            proportions = np.array([p if p * len(idx_j) < N / num_partitions else 0 for p, idx_j in zip(proportions, partition_indices)])\n",
    "\n",
    "            proportions = proportions / np.sum(proportions)\n",
    "            proportions = (np.cumsum(proportions) * len(idx_label)).astype(int)[:-1]\n",
    "\n",
    "            partition_indices = [idx_j + idx.tolist() for idx_j, idx in zip(partition_indices, np.split(idx_label, proportions))]\n",
    "            min_size = min([len(idx_j) for idx_j in partition_indices])\n",
    "        \n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "\n",
    "def apply_gaussian_noise(tensor, std_dev):\n",
    "    noise = torch.randn_like(tensor) * std_dev\n",
    "    return tensor + noise\n",
    "\n",
    "# Hàm đảo ngược chuẩn hóa\n",
    "def unnormalize_image(image_tensor, mean, std):\n",
    "    # Đảo ngược Normalize: (image * std) + mean\n",
    "    for t, m, s in zip(image_tensor, mean, std):\n",
    "        t.mul_(s).add_(m)  # Thực hiện từng kênh\n",
    "    return image_tensor\n",
    "\n",
    "# Hàm hiển thị ảnh từ một tensor\n",
    "def display_image(image_tensor, mean, std):\n",
    "    # Đảo ngược chuẩn hóa\n",
    "    image_tensor = unnormalize_image(image_tensor, mean, std)\n",
    "    # Chuyển tensor thành NumPy array và điều chỉnh thứ tự kênh màu (CHW -> HWC)\n",
    "    image_numpy = image_tensor.permute(1, 2, 0).numpy()\n",
    "    # Cắt giá trị ảnh về phạm vi [0, 1] để hiển thị đúng\n",
    "    image_numpy = image_numpy.clip(0, 1)\n",
    "    # Trả về ảnh NumPy\n",
    "    return image_numpy\n",
    "\n",
    "def prepare_noise_based_imbalance(num_partitions: int, batch_size: int, val_ratio: float = 0.1, sigma: float = 0.05, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Chia du lieu ngau nhien va deu cho cac ben, sau do them noise vao cac ben\n",
    "    moi ben i co noise khac nhau Gauss(0, sigma*i/N)\n",
    "    \"\"\"\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    indices = trainset.indices\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    partition_indices = np.array_split(indices, num_partitions)\n",
    "\n",
    "    train_partitions = []\n",
    "\n",
    "    for i, part_indices in enumerate(partition_indices):\n",
    "        partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "        partition_set = Subset(trainset.dataset, part_indices)\n",
    "        \n",
    "        noisy_samples = [apply_gaussian_noise(sample[0], partition_std_dev) for sample in partition_set]\n",
    "        noisy_dataset = [(noisy_samples[j], trainset.dataset[part_indices[j]][1]) for j in range(len(part_indices))]\n",
    "        # train_partitions.append((noisy_samples, [sample[1] for sample in partition_set]))\n",
    "        train_partitions.append(noisy_dataset)\n",
    "    trainloaders = [DataLoader(train_partitions[i], batch_size=batch_size, shuffle=True, num_workers=4) for i in range(num_partitions)]\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "####\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "    #Lưu ảnh nhiễu vào running_outputs\n",
    "    # Mean và std từ Normalize\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    # Tạo thư mục lưu ảnh nếu chưa tồn tại\n",
    "    output_dir = \"running_outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Khởi tạo một lưới 10x6 để hiển thị ảnh\n",
    "    fig, axes = plt.subplots(10, 6, figsize=(15, 25))\n",
    "\n",
    "    # Duyệt qua 60 trainloaders và hiển thị ảnh đầu tiên\n",
    "    for i, trainloader in enumerate(trainloaders[:num_partitions]):\n",
    "        # Lấy ảnh đầu tiên từ trainloader\n",
    "        image_tensor = trainloader.dataset[0][0].clone()  # Clone để tránh thay đổi dữ liệu gốc\n",
    "        \n",
    "        # Tìm vị trí hàng, cột trong lưới\n",
    "        row, col = divmod(i, 6)\n",
    "        plt.sca(axes[row, col])  # Đặt trục hiện tại là vị trí hàng, cột trong lưới\n",
    "        \n",
    "        # Hiển thị ảnh\n",
    "        image_numpy = display_image(image_tensor, mean, std)\n",
    "        axes[row, col].imshow(image_numpy)\n",
    "        axes[row, col].axis('off')\n",
    "    plt.title(f\"Noise image with sigma from {sigma * 1 / num_partitions} to {sigma}\")\n",
    "    # Điều chỉnh layout để không bị chồng lấn\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Lưu ảnh thay vì hiển thị\n",
    "    output_path = os.path.join(output_dir, \"image_noise.png\")\n",
    "    plt.savefig(output_path, dpi=300)  # Lưu ảnh với chất lượng cao\n",
    "\n",
    "    plt.close()  # Đóng figure\n",
    "\n",
    "    print(f\"Ảnh đã được lưu tại {output_path}\")\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "###\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "def prepare_quantity_skew_dirichlet(num_partitions: int, batch_size: int, val_ratio: float = 0.1, beta: float = 10, seed: int = 42):\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    all_indices = trainset.indices\n",
    "\n",
    "    min_size = 0\n",
    "    while min_size < 1:\n",
    "        proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "        proportions = (np.cumsum(proportions) * len(all_indices)).astype(int)[:-1]\n",
    "\n",
    "        partition_indices = np.split(all_indices, proportions)\n",
    "\n",
    "        min_size = min([len(partition) for partition in partition_indices])\n",
    "        print('Partition sizes:', [len(partition) for partition in partition_indices])\n",
    "        print('Min partition size:', min_size)\n",
    "\n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "def  load_datasets(\n",
    "    config: DictConfig,\n",
    "    num_clients: int,\n",
    "    val_ratio: float = 0.1,\n",
    "    seed: Optional[int] = 42,\n",
    ") -> Tuple[List[DataLoader], List[DataLoader], DataLoader]:\n",
    "    \"\"\"Create the dataloaders to be fed into the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config: DictConfig\n",
    "        Parameterises the dataset partitioning process\n",
    "    num_clients : int\n",
    "        The number of clients that hold a part of the data\n",
    "    val_ratio : float, optional\n",
    "        The ratio of training data that will be used for validation (between 0 and 1),\n",
    "        by default 0.1\n",
    "    seed : int, optional\n",
    "        Used to set a fix seed to replicate experiments, by default 42\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[DataLoader, DataLoader, DataLoader]\n",
    "        The DataLoaders for training, validation, and testing.\n",
    "    \"\"\"\n",
    "    print(f\"Dataset partitioning config: {config}\")\n",
    "    batch_size = -1\n",
    "    print('config:' , config)\n",
    "    if \"batch_size\" in config:\n",
    "        batch_size = config.batch_size\n",
    "    elif \"batch_size_ratio\" in config:\n",
    "        batch_size_ratio = config.batch_size_ratio\n",
    "    else:\n",
    "        raise ValueError\n",
    "    partitioning = \"\"\n",
    "    \n",
    "    if \"partitioning\" in config:\n",
    "        partitioning = config.partitioning\n",
    "\n",
    "    # partition the data\n",
    "    if partitioning == \"imbalance_label\":\n",
    "        return prepare_partitioned_dataset(num_clients, batch_size, val_ratio, config.labels_per_client, config.seed)\n",
    "\n",
    "    if partitioning == \"imbalance_label_dirichlet\":\n",
    "        return prepare_imbalance_label_dirichlet(num_clients, batch_size, val_ratio, config.alpha, config.seed)\n",
    "\n",
    "    if partitioning == \"noise_based_imbalance\":\n",
    "        return prepare_noise_based_imbalance(num_clients, batch_size, val_ratio, config.sigma, config.seed)\n",
    "\n",
    "    if partitioning == \"quantity_skew_dirichlet\":\n",
    "        return prepare_quantity_skew_dirichlet(num_clients, batch_size, val_ratio, config.alpha, config.seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_fig_with_incremental_name(base_path):\n",
    "    if not os.path.exists(base_path):\n",
    "        plt.savefig(base_path)\n",
    "    else:\n",
    "        base_name, ext = os.path.splitext(base_path)\n",
    "        counter = 1\n",
    "        new_path = f\"{base_name} ({counter}){ext}\"\n",
    "        while os.path.exists(new_path):\n",
    "            counter += 1\n",
    "            new_path = f\"{base_name} ({counter}){ext}\"\n",
    "        plt.savefig(new_path)\n",
    "    plt.close()\n",
    "\n",
    "# Sử dụng hàm\n",
    "# save_fig_with_incremental_name('running_outputs/accuracy_summary.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        # self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)  # Output corresponds to num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=True)  # Sử dụng ResNet50\n",
    "\n",
    "        # Điều chỉnh lại lớp phân loại (fc) với cấu trúc mới\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Flatten(),                     # Flatten đầu vào\n",
    "            nn.Linear(2048, 512),             # Chuyển từ 2048 (đặc trưng đầu ra của ResNet50) xuống 512\n",
    "            nn.ReLU(inplace=True),            # Hàm kích hoạt ReLU\n",
    "            nn.Linear(512, 128),              # Chuyển từ 512 xuống 128\n",
    "            nn.ReLU(inplace=True),            # Hàm kích hoạt ReLU\n",
    "            nn.Linear(128, num_classes)       # Output với số lớp bằng num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)  # Truyền dữ liệu qua mô hình ResNet50\n",
    "    \n",
    "\n",
    "class VGG11Model(nn.Module):\n",
    "    # Implement VGG11 model for transfer learning\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = models.vgg11(pretrained=True)\n",
    "        \n",
    "        # Freeze the convolutional base\n",
    "        # for param in self.model.features.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        # Replace avgpool with AdaptiveAvgPool2d\n",
    "        self.model.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Replace the classifier with a new one\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)  # Output corresponds to num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim \n",
    "import copy\n",
    "import random \n",
    "import numpy as np\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def federated_train(trainloaders, valloaders, testloader, config):\n",
    "    model = ResNet18(num_classes=2)\n",
    "    # model = ResNet50(num_classes=2)\n",
    "    # model = VGG11Model(num_classes=2)\n",
    "    global_model = copy.deepcopy(model)  # Bản sao mô hình toàn cục\n",
    "        \n",
    "    num_rounds = config.num_rounds  # Số vòng huấn luyện\n",
    "    accs = []\n",
    "\n",
    "    accs.append(evaluate(global_model, testloader)) #huan luyen 1 lan trc o server\n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"Round {round_num + 1}/{num_rounds}\")\n",
    "        start = time.time()\n",
    "        a_list = []\n",
    "        d_list = []\n",
    "        n_list = []\n",
    "        # Chọn các client tham gia vào mỗi round\n",
    "        selected_clients = select_clients(trainloaders, config.clients_per_round)\n",
    "        \n",
    "        # Huấn luyện trên các client đã chọn\n",
    "        for client in selected_clients:\n",
    "            a_i, d_i = local_train(client, global_model, config, trainloaders)\n",
    "            a_list.append(a_i)\n",
    "            d_list.append(d_i)\n",
    "            n_list.append(len(trainloaders[client].dataset))\n",
    "            \n",
    "        total_n = sum(n_list)\n",
    "\n",
    "\n",
    "        d_total_round = copy.deepcopy(global_model.state_dict())\n",
    "        for key in d_total_round:\n",
    "            d_total_round[key] = 0.0\n",
    "        \n",
    "        for i in range(len(selected_clients)):\n",
    "            d_para = d_list[i]\n",
    "            for key in d_para:\n",
    "                d_total_round[key] += d_para[key] * n_list[i] / total_n\n",
    "        \n",
    "        #Update global model\n",
    "        coeff = 0.0\n",
    "        for i in range(len(selected_clients)):\n",
    "            coeff += a_list[i] * n_list[i] / total_n\n",
    "        \n",
    "        updated_model = global_model.state_dict()\n",
    "        for key in updated_model:\n",
    "            if updated_model[key].type() == 'torch.LongTensor':\n",
    "                updated_model[key] -= (coeff * d_total_round[key]).type(torch.LongTensor)\n",
    "            elif updated_model[key].type() == 'torch.cuda.LongTensor':\n",
    "                    updated_model[key] -= (coeff * d_total_round[key]).type(torch.cuda.LongTensor)\n",
    "            else:\n",
    "                #print(updated_model[key].type())\n",
    "                #print((coeff*d_total_round[key].type()))\n",
    "                updated_model[key] -= coeff * d_total_round[key]\n",
    "        global_model.load_state_dict(updated_model)    \n",
    "\n",
    "        # # Cập nhật mô hình toàn cục sử dụng FedNova\n",
    "        # global_model = fednova_update(global_model, len_data_local_select, local_deltas, taus)\n",
    "        \n",
    "        # Đánh giá mô hình trên tập kiểm tra\n",
    "        acc = evaluate(global_model, testloader)\n",
    "        accs.append(acc)\n",
    "        # Điều chỉnh learning rate theo độ chính xác\n",
    "        # if acc > 70.0:\n",
    "        #     config.learning_rate = 1e-5\n",
    "        #     print(f\"Accuracy > 80%, decreasing learning rate to {config.learning_rate}\")\n",
    "        # elif acc > 65.0:\n",
    "        #     config.learning_rate = 1e-4\n",
    "        #     print(f\"Accuracy > 70%, decreasing learning rate to {config.learning_rate}\")\n",
    "        end = time.time()\n",
    "        print(f'Time for round {round_num + 1}: ', end-start)\n",
    "    print('accuracies: ', accs)\n",
    "    plt.plot(range(0, num_rounds + 1), accs, marker='o', label='Accuracy')\n",
    "    plt.xlabel('Round')\n",
    "    plt.xticks(range(0, num_rounds + 1, 10))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('FedNova on ResNet18 over Rounds')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    # plt.savefig('running_outputs/accuracy_summary.png')\n",
    "    save_fig_with_incremental_name('running_outputs/accuracy_summary.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def select_clients(trainloaders, clients_per_round):\n",
    "    \"\"\"Chọn ngẫu nhiên một số client tham gia huấn luyện trong mỗi round.\"\"\"\n",
    "    # Số lượng client có sẵn\n",
    "    total_clients = len(trainloaders)\n",
    "    # Chọn ngẫu nhiên một số client\n",
    "    selected_clients = random.sample(range(total_clients), clients_per_round)\n",
    "    return selected_clients\n",
    "\n",
    "def local_train(client, global_model, config, trainloader):\n",
    "    \"\"\"Huấn luyện mô hình trên một client cụ thể.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = copy.deepcopy(global_model).to(device) # Sử dụng bản sao mô hình toàn cục\n",
    "    net.train()\n",
    "    \n",
    "    # Sử dụng SGD với learning rate = 1e-3 và momentum = 0.9\n",
    "    optimizer = optim.SGD(\n",
    "        # net.parameters(),\n",
    "        filter(lambda p: p.requires_grad, net.parameters()), \n",
    "        lr=config.learning_rate, \n",
    "        momentum=config.momentum)\n",
    "    tau = 0\n",
    "    print(f\"Training on client {client}, device: {device}, learning_rate={config.learning_rate}\")\n",
    "    # Huấn luyện mô hình trên client\n",
    "    for epoch in range(config.num_epochs):\n",
    "        for batch_idx, (data, target) in enumerate(trainloader[client]):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tau += 1  # Tăng số lần cập nhật\n",
    "\n",
    "    a_i = (tau-config.momentum * (1-pow(config.momentum, tau)) / (1 - config.momentum)) / (1 - config.momentum)\n",
    "    global_model_para = global_model.state_dict()\n",
    "    net_para = net.state_dict()\n",
    "    norm_grad = copy.deepcopy(global_model.state_dict())\n",
    "    for key in norm_grad:\n",
    "        norm_grad[key] = torch.true_divide(global_model_para[key] - net_para[key].to('cpu'), a_i)\n",
    "    \n",
    "\n",
    "    return a_i, norm_grad\n",
    "\n",
    "def evaluate(model, testloader):\n",
    "    \"\"\"Đánh giá mô hình trên tập kiểm tra.\"\"\"\n",
    "    model.eval()  # Chuyển sang chế độ đánh giá\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Config:\n",
      "{'num_clients': 4, 'num_epochs': 1, 'batch_size': 10, 'clients_per_round': 2, 'fraction_fit': 0.1, 'learning_rate': 0.001, 'num_rounds': 1, 'partitioning': 'imbalance_label', 'dataset_name': 'chest_xray', 'dataset_seed': 42, 'alpha': 0.5, 'sigma': 0.1, 'labels_per_client': 1, 'momentum': 0.9, 'weight_decay': 1e-05, 'dataset': {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}}\n",
      "Dataset partitioning config: {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}\n",
      "config: {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}\n",
      "[2, 2]\n",
      "[[0], [1], [0], [1]]\n",
      "Partition 0 class distribution: {0: 26}\n",
      "Partition 1 class distribution: {1: 70}\n",
      "Partition 2 class distribution: {0: 25}\n",
      "Partition 3 class distribution: {1: 70}\n",
      "Number of train samples: 191, val samples: 0, test samples: 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 46.51%\n",
      "Round 1/1\n",
      "Training on client 0, device: cuda, learning_rate=0.001\n",
      "Training on client 1, device: cuda, learning_rate=0.001\n",
      "Test Accuracy: 46.51%\n",
      "Time for round 1:  9.408247947692871\n",
      "accuracies:  [46.51162790697674, 46.51162790697674]\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf  # Thêm OmegaConf\n",
    "import os\n",
    "\n",
    "# def load_config(config_file):\n",
    "#     \"\"\"Load configuration using OmegaConf (DictConfig)\"\"\"\n",
    "#     # Dùng OmegaConf để load file config.yaml\n",
    "#     config = OmegaConf.load(config_file)\n",
    "#     return config\n",
    "def load_config():\n",
    "    \"\"\"Load configuration using OmegaConf from a dictionary.\"\"\"\n",
    "    config_dict = {\n",
    "        \"num_clients\": 4,\n",
    "        \"num_epochs\": 1,\n",
    "        \"batch_size\": 10,\n",
    "        \"clients_per_round\": 2,\n",
    "        \"fraction_fit\": 0.1,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"num_rounds\": 1,\n",
    "        \"partitioning\": \"imbalance_label\",\n",
    "        \"dataset_name\": \"chest_xray\",\n",
    "        \"dataset_seed\": 42,\n",
    "        \"alpha\": 0.5,\n",
    "        \"sigma\": 0.1,\n",
    "        \"labels_per_client\": 1,  # only used when partitioning is label quantity\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 0.00001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"${dataset_name}\",\n",
    "            \"partitioning\": \"${partitioning}\",\n",
    "            \"batch_size\": \"${batch_size}\",  # batch_size = batch_size_ratio * total_local_data_size\n",
    "            \"val_split\": 0.0,\n",
    "            \"seed\": \"${dataset_seed}\",\n",
    "            \"alpha\": \"${alpha}\",\n",
    "            \"sigma\": \"${sigma}\",\n",
    "            \"labels_per_client\": \"${labels_per_client}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Chuyển đổi dictionary thành DictConfig\n",
    "    config = OmegaConf.create(config_dict)\n",
    "\n",
    "    return config\n",
    "\n",
    "def main():\n",
    "    # Parse arguments\n",
    "    # args = parse_args()\n",
    "\n",
    "    # Load configuration file\n",
    "    config = load_config()  # Trả về DictConfig\n",
    "\n",
    "    # Kiểm tra các tham số được thay thế chính xác\n",
    "    print(\"Loaded Config:\")\n",
    "    print(config)\n",
    "\n",
    "    # Load dataset\n",
    "    # trainloaders, valloaders, testloader = load_datasets(config.dataset.name, args.num_clients)\n",
    "    trainloaders, valloaders, testloader = load_datasets(\n",
    "        config=config.dataset,\n",
    "        num_clients=config.num_clients,\n",
    "        val_ratio=config.dataset.val_split,\n",
    "    )\n",
    "\n",
    "    # Train federated model\n",
    "    federated_train(trainloaders, valloaders, testloader, config)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Config:\n",
      "{'num_clients': 4, 'num_epochs': 1, 'batch_size': 10, 'clients_per_round': 2, 'fraction_fit': 0.1, 'learning_rate': 0.001, 'num_rounds': 2, 'partitioning': 'imbalance_label', 'dataset_name': 'chest_xray', 'dataset_seed': 42, 'alpha': 0.5, 'sigma': 0.1, 'labels_per_client': 1, 'momentum': 0.9, 'weight_decay': 1e-05, 'dataset': {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}}\n",
      "Dataset partitioning config: {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}\n",
      "config: {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}\n",
      "[2, 2]\n",
      "[[0], [1], [0], [1]]\n",
      "Partition 0 class distribution: {0: 26}\n",
      "Partition 1 class distribution: {1: 70}\n",
      "Partition 2 class distribution: {0: 25}\n",
      "Partition 3 class distribution: {1: 70}\n",
      "Number of train samples: 191, val samples: 0, test samples: 129\n",
      "Test Accuracy: 31.78%\n",
      "Round 1/2\n",
      "Training on client 0, device: cuda, learning_rate=0.001\n",
      "Training on client 3, device: cuda, learning_rate=0.001\n",
      "Test Accuracy: 37.21%\n",
      "Time for round 1:  4.233982801437378\n",
      "Round 2/2\n",
      "Training on client 0, device: cuda, learning_rate=0.001\n",
      "Training on client 3, device: cuda, learning_rate=0.001\n",
      "Test Accuracy: 46.51%\n",
      "Time for round 2:  4.200951814651489\n",
      "accuracies:  [31.782945736434108, 37.2093023255814, 46.51162790697674]\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf  # Thêm OmegaConf\n",
    "import os\n",
    "\n",
    "# def load_config(config_file):\n",
    "#     \"\"\"Load configuration using OmegaConf (DictConfig)\"\"\"\n",
    "#     # Dùng OmegaConf để load file config.yaml\n",
    "#     config = OmegaConf.load(config_file)\n",
    "#     return config\n",
    "def load_config():\n",
    "    \"\"\"Load configuration using OmegaConf from a dictionary.\"\"\"\n",
    "    config_dict = {\n",
    "        \"num_clients\": 4,\n",
    "        \"num_epochs\": 1,\n",
    "        \"batch_size\": 10,\n",
    "        \"clients_per_round\": 2,\n",
    "        \"fraction_fit\": 0.1,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"num_rounds\": 2,\n",
    "        \"partitioning\": \"imbalance_label\",\n",
    "        \"dataset_name\": \"chest_xray\",\n",
    "        \"dataset_seed\": 42,\n",
    "        \"alpha\": 0.5,\n",
    "        \"sigma\": 0.1,\n",
    "        \"labels_per_client\": 1,  # only used when partitioning is label quantity\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 0.00001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"${dataset_name}\",\n",
    "            \"partitioning\": \"${partitioning}\",\n",
    "            \"batch_size\": \"${batch_size}\",  # batch_size = batch_size_ratio * total_local_data_size\n",
    "            \"val_split\": 0.0,\n",
    "            \"seed\": \"${dataset_seed}\",\n",
    "            \"alpha\": \"${alpha}\",\n",
    "            \"sigma\": \"${sigma}\",\n",
    "            \"labels_per_client\": \"${labels_per_client}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Chuyển đổi dictionary thành DictConfig\n",
    "    config = OmegaConf.create(config_dict)\n",
    "\n",
    "    return config\n",
    "\n",
    "def main():\n",
    "    # Parse arguments\n",
    "    # args = parse_args()\n",
    "\n",
    "    # Load configuration file\n",
    "    config = load_config()  # Trả về DictConfig\n",
    "\n",
    "    # Kiểm tra các tham số được thay thế chính xác\n",
    "    print(\"Loaded Config:\")\n",
    "    print(config)\n",
    "\n",
    "    # Load dataset\n",
    "    # trainloaders, valloaders, testloader = load_datasets(config.dataset.name, args.num_clients)\n",
    "    trainloaders, valloaders, testloader = load_datasets(\n",
    "        config=config.dataset,\n",
    "        num_clients=config.num_clients,\n",
    "        val_ratio=config.dataset.val_split,\n",
    "    )\n",
    "\n",
    "    # Train federated model\n",
    "    federated_train(trainloaders, valloaders, testloader, config)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
